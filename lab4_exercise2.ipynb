{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "The goal of this exercise is to build a straightforward machine learning pipeline for a problem with more than two classes.  A lot of the data preprocessing has already been done, so the main focus of this exercise is to become familiar with loading data, training a model, doing inference, and analyzing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "For example, here's the first couple rows of the dataset:\n",
    "\n",
    "| Source IP    |  Source Port |  Destination IP   |  Destination Port |  Protocol |  Flow Duration |  Flow Bytes/s |  Flow Packets/s |  Flow IAT Mean |  Flow IAT Std |  Flow IAT Max |  Flow IAT Min | Fwd IAT Mean |  Fwd IAT Std |  Fwd IAT Max |  Fwd IAT Min | Bwd IAT Mean |  Bwd IAT Std |  Bwd IAT Max |  Bwd IAT Min | Active Mean |  Active Std |  Active Max |  Active Min | Idle Mean |  Idle Std |  Idle Max |  Idle Min | label |\n",
    "|--------------|--------------|-------------------|-------------------|-----------|----------------|---------------|-----------------|----------------|---------------|---------------|---------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|-------------|-------------|-------------|-------------|-----------|-----------|-----------|-----------|-------|\n",
    "| 10\\.0\\.2\\.15 | 57188        | 82\\.161\\.239\\.177 | 110               | 6         | 7248168        | 21126\\.02798  | 29\\.11080428    | 34515\\.08571   | 273869\\.2625  | 3897923       | 5             | 89483\\.55556 | 437167\\.5917 | 3898126      | 29           | 56614\\.03906 | 349855\\.1098 | 3898131      | 7            | 0           | 0           | 0           | 0           | 0         | 0         | 0         | 0         | AUDIO |\n",
    "| 10\\.0\\.2\\.15 | 57188        | 82\\.161\\.239\\.177 | 110               | 6         | 5157723        | 1052\\.790156  | 3\\.683796125    | 286540\\.1667   | 878838\\.5256  | 3743359       | 135           | 644715\\.375  | 1272066\\.058 | 3743562      | 509          | 568901\\.6667 | 1209110\\.287 | 3743573      | 451          | 0           | 0           | 0           | 0           | 0         | 0         | 0         | 0         | AUDIO |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV data as a Pandas dataframe\n",
    "# The data is in 'data/exercise2/TOR_TimeBasedFeatures-10s-Layer2.csv'\n",
    "\n",
    "# CODE HERE\n",
    "csv_data = pd.read_csv('data/exercise2/TOR_TimeBasedFeatures-10s-Layer2.csv')\n",
    "\n",
    "# Create data and labels that can be used by sklearn's 'train_test_split'\n",
    "# Create the labels\n",
    "\n",
    "# CODE HERE\n",
    "labels = csv_data['label']\n",
    "\n",
    "# Create the data\n",
    "# -Keep just the numeric features (i.e., those features between 'Flow Duration' and 'Idle Min')\n",
    "# -Make sure not to keep the labels\n",
    "\n",
    "# CODE HERE\n",
    "data = csv_data.iloc[:,5:28]\n",
    "\n",
    "# You should now have data and labels that can be used by sklearn's 'train_test_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a single train/test split for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick 50% of the data for the training set, and keep the remaining 50% for the test set\n",
    "# Use sklearn's 'train_test_split'\n",
    "# CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/EE379K/EE379K-Lab4/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier using default hyperparameters\n",
    "# Hint: Not counting any import statements, this can be done in a single line of code\n",
    "# CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the classifier on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8145201392342118\n",
      "Confusion Matrix:\n",
      "[[ 287   82    0    3    4    0   12    3]\n",
      " [  57  635   33    8    6    1   52    1]\n",
      " [   6   84   47    1    0    0    5    6]\n",
      " [   7   27    3  372    4    0   28    1]\n",
      " [   4   53    6   13   42    0   17    2]\n",
      " [   4    7    0    2    0  505    8    0]\n",
      " [  14   90    5   32    5   11  266    3]\n",
      " [   2   22    4    4    0    3    1 1122]]\n",
      "Confusion Matrix with Labels:\n",
      "                 p:AUDIO  p:BROWSING  p:CHAT  p:FILE-TRANSFER  p:MAIL  p:P2P  \\\n",
      "t:AUDIO              287          82       0                3       4      0   \n",
      "t:BROWSING            57         635      33                8       6      1   \n",
      "t:CHAT                 6          84      47                1       0      0   \n",
      "t:FILE-TRANSFER        7          27       3              372       4      0   \n",
      "t:MAIL                 4          53       6               13      42      0   \n",
      "t:P2P                  4           7       0                2       0    505   \n",
      "t:VIDEO               14          90       5               32       5     11   \n",
      "t:VOIP                 2          22       4                4       0      3   \n",
      "\n",
      "                 p:VIDEO  p:VOIP  \n",
      "t:AUDIO               12       3  \n",
      "t:BROWSING            52       1  \n",
      "t:CHAT                 5       6  \n",
      "t:FILE-TRANSFER       28       1  \n",
      "t:MAIL                17       2  \n",
      "t:P2P                  8       0  \n",
      "t:VIDEO              266       3  \n",
      "t:VOIP                 1    1122  \n"
     ]
    }
   ],
   "source": [
    "# Predict the labels on the test set\n",
    "\n",
    "# CODE HERE\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Use accuracy and a confusion matrix to measure performance\n",
    "# Hint: Use sklearn's built-in metrics\n",
    "\n",
    "# CODE HERE\n",
    "from sklearn import metrics\n",
    "labels_uniq = csv_data['label'].unique()\n",
    "acc = metrics.accuracy_score(y_test, preds)\n",
    "matrix = metrics.confusion_matrix(y_test, preds, labels=labels_uniq)\n",
    "mat_df = pd.DataFrame(\n",
    "    matrix,\n",
    "    index=['t:'+x for x in labels_uniq],\n",
    "    columns=['p:'+x for x in labels_uniq]\n",
    ")\n",
    "print('Accuracy: {}'.format(acc))\n",
    "print('Confusion Matrix:\\n{}'.format(matrix))\n",
    "print('Confusion Matrix with Labels:\\n{}'.format(mat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature\t\tImportance\n",
      "Active Mean\t0.10515457310120781\n",
      "Fwd IAT Min\t0.09589100740451892\n",
      "Flow IAT Min\t0.08354734280312069\n",
      "Bwd IAT Std\t0.07723398973690583\n",
      "Active Min\t0.07689374865493556\n",
      "Idle Mean\t0.06969341662197867\n",
      "Fwd IAT Mean\t0.0658461807116965\n",
      "Bwd IAT Min\t0.06448185662688093\n",
      "Fwd IAT Std\t0.06325935819009602\n",
      "Active Std\t0.05477230498503107\n",
      "Bwd IAT Max\t0.051339436169573024\n",
      "Flow IAT Max\t0.05133361534142815\n",
      "Bwd IAT Mean\t0.04605061147048475\n",
      "Fwd IAT Max\t0.04498200340507924\n",
      "Active Max\t0.03393568389701756\n",
      "Idle Std\t0.0033827536084262244\n",
      "Idle Min\t0.002773785217306177\n",
      "Idle Max\t0.0\n"
     ]
    }
   ],
   "source": [
    "# Determine important features\n",
    "\n",
    "# CODE HERE\n",
    "ft_imps = [ft for ft in zip(data.columns[5:28], model.feature_importances_)]\n",
    "ft_imps.sort(key=lambda ft: ft[1], reverse=True)\n",
    "print('Feature\\t\\tImportance')\n",
    "for ft in ft_imps: print('{}\\t{}'.format(ft[0].strip(), ft[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1) What is the overall accuracy using the default parameters?  \n",
    "\n",
    "The overall accuracy using the default parameters is about 0.8145, or 81.45%.\n",
    "\n",
    "2) What is the confusion matrix for the tested approach?  What are the classes where the model performs well?  What are the classes where the model performs poorly?\n",
    "\n",
    "The confusion matrix, shown above, is copied here for convenience (labeled confusion matrix is left above):\n",
    "```\n",
    "[[ 287   82    0    3    4    0   12    3]\n",
    " [  57  635   33    8    6    1   52    1]\n",
    " [   6   84   47    1    0    0    5    6]\n",
    " [   7   27    3  372    4    0   28    1]\n",
    " [   4   53    6   13   42    0   17    2]\n",
    " [   4    7    0    2    0  505    8    0]\n",
    " [  14   90    5   32    5   11  266    3]\n",
    " [   2   22    4    4    0    3    1 1122]]\n",
    "```\n",
    "The model seems to perform very well for 'VOIP' and 'P2P.' On the other hand, some classes where the model performs poorly are 'CHAT' and 'MAIL.'\n",
    "\n",
    "3) What are the top 5 most important features?\n",
    "\n",
    "The top 5 most important features, as shown above, are: Active Mean, Fwd IAT Min, Flow IAT Min, Bwd IAT Std, and Active Min.\n",
    "\n",
    "4) What hyperparameters could you tune in the random forest to improve performance? What is the best accuracy you can attain?\n",
    "\n",
    "There are a lot of hyperparameters that can be changed to tune the model and to improve performance. Some of these include: n_estimators (number of trees in the forest), max_depth (max depth of a tree), and min_samples_leaf (minimum number of samples to be considered a leaf). After a few tries of modifying these parameters, the best accuracy obtained was about 0.8277 or 82.77%, as shown below. However, if the model is extremely overfit to the test data set, then an accuracy of or near 100% could be possible, although extremely unlikely.\n",
    "\n",
    "5) Bonus: How would you improve the pipeline above to automatically tune the hyperparameters?  How would you improve the pipeline to use multiple train/test splits?\n",
    "\n",
    "Combining lists of values for several hyperparameters into a dictionary and performing cross validation on the combinations can help automatically tune the hyperparameters. For example, using sklearn.model_selection.GridSearchCV will return a model with the best performing hyperparameter values. Additionally, using multiple train/test splits can also help automatically tune the hyperparameters. In this case, using sklearn.model_selection.StratifiedKFold can be helpful, since it also preserves the percentage of samples for each class. K-fold cross validation can then help determine how to split the data for the model to perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8276976628543014\n"
     ]
    }
   ],
   "source": [
    "# tuning\n",
    "tune_model = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=100,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "tune_preds = tune_model.predict(X_test)\n",
    "print('Accuracy: {}'.format(metrics.accuracy_score(y_test, tune_preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
