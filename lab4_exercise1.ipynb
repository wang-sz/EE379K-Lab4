{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "Here is an example of the first couple rows from the data:\n",
    "\n",
    "| id | dur       | proto | service | state | spkts | dpkts | sbytes | dbytes | rate        | sttl | dttl | sload     | dload | sloss | dloss | sinpkt | dinpkt | sjit | djit | swin | stcpb | dtcpb | dwin | tcprtt | synack | ackdat | smean | dmean | trans\\_depth | response\\_body\\_len | ct\\_srv\\_src | ct\\_state\\_ttl | ct\\_dst\\_ltm | ct\\_src\\_dport\\_ltm | ct\\_dst\\_sport\\_ltm | ct\\_dst\\_src\\_ltm | is\\_ftp\\_login | ct\\_ftp\\_cmd | ct\\_flw\\_http\\_mthd | ct\\_src\\_ltm | ct\\_srv\\_dst | is\\_sm\\_ips\\_ports | attack\\_cat | label |\n",
    "|----|-----------|-------|---------|-------|-------|-------|--------|--------|-------------|------|------|-----------|-------|-------|-------|--------|--------|------|------|------|-------|-------|------|--------|--------|--------|-------|-------|--------------|---------------------|--------------|----------------|--------------|---------------------|---------------------|-------------------|----------------|--------------|---------------------|--------------|--------------|--------------------|-------------|-------|\n",
    "| 1  | 0\\.000011 | udp   | \\-      | INT   | 2     | 0     | 496    | 0      | 90909\\.0902 | 254  | 0    | 180363632 | 0     | 0     | 0     | 0\\.011 | 0      | 0    | 0    | 0    | 0     | 0     | 0    | 0      | 0      | 0      | 248   | 0     | 0            | 0                   | 2            | 2              | 1            | 1                   | 1                   | 2                 | 0              | 0            | 0                   | 1            | 2            | 0                  | Normal      | 0     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data corresponding to exercise 1\n",
    "# Create two separate pandas dataframes for the training and test data\n",
    "# For each dataframe, import the following CSV data\n",
    "# training set: 'data/exercise1/UNSW_NB15_training-set.csv'\n",
    "# test set: 'data/exercise1/UNSW_NB15_testing-set.csv'\n",
    "\n",
    "# CODE HERE\n",
    "train = pd.read_csv('data/exercise1/UNSW_NB15_training-set.csv',index_col='id')\n",
    "test = pd.read_csv('data/exercise1/UNSW_NB15_testing-set.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 \n",
    "Keep the two dataframes separate and create train/test data and labels.  This will be used to experiment with the case where there are different types of activities in the training versus test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all the following operations in the cell on both the dataframes separately\n",
    "# 1) Keep only the datapoints where the 'attack_cat' column is equal to either 'Normal' or 'Fuzzers'\n",
    "# CODE HERE\n",
    "train_data = train.loc[(train['attack_cat'] == 'Normal') | (train['attack_cat'] == 'Fuzzers')]\n",
    "test_data = test.loc[(test['attack_cat'] == 'Normal') | (test['attack_cat'] == 'Fuzzers')]\n",
    "\n",
    "# 2) Get the labels from the dataframe (i.e., the values in the 'attack_cat' column)\n",
    "# CODE HERE\n",
    "train_labels = train_data['attack_cat']\n",
    "test_labels = test_data['attack_cat']\n",
    "\n",
    "# 3) Keep only the features we care about for this experiment.\n",
    "# We only care about the numerical features between column 'spkts' and 'is_sm_ips_ports' (inclusive)\n",
    "# CODE HERE\n",
    "train_data = train_data.iloc[:,4:42]\n",
    "test_data = test_data.iloc[:,4:42]\n",
    "\n",
    "# You should now have four inputs usable for scikit-learn:\n",
    "# training data -> train_data\n",
    "# training labels -> train_labels\n",
    "# test data -> test_data\n",
    "# test labels -> test_labels\n",
    "# Hint: You may have to add some minor code in the above to get the data ready for scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 \n",
    "Create a new training/test split by combining the dataframes into one.  Then split the dataframe randomly into train/test data and labels.  This will be used to experiment with the case where there are largely the same types of activities in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes into a single dataframe, then do the following\n",
    "# CODE HERE\n",
    "com = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# 1) Keep only the datapoints where the 'attack_cat' column is equal to either 'Normal' or 'Fuzzers'\n",
    "# CODE HERE\n",
    "com_data = com.loc[(com['attack_cat'] == 'Normal') | (com['attack_cat'] == 'Fuzzers')]\n",
    "\n",
    "# 2) Get the labels from the dataframe (i.e., the values in the 'attack_cat' column)\n",
    "# CODE HERE\n",
    "com_labels = com_data['attack_cat']\n",
    "\n",
    "# 3) Keep only features we care about for this experiment.\n",
    "# We only care about the numerical features between column 'spkts' and 'is_sm_ips_ports' (inclusive)\n",
    "# CODE HERE\n",
    "com_data = com_data.iloc[:,4:42]\n",
    "\n",
    "# 4) Create a random split; put 50% of the data into the training set and the other 50% into the test set\n",
    "# Use scikit-learn's 'train_test_split'\n",
    "# Hint: You may have to add some minor code in the above to get the data ready for scikit-learn's 'train_test_split'\n",
    "# CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(com_data, com_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# You should now have four inputs usable for scikit-learn that are distinct from the inputs you created in Part 1.\n",
    "# The inputs should correspond to:\n",
    "# training data -> X_train\n",
    "# training labels -> y_train\n",
    "# test data -> X_test\n",
    "# test labels -> y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/EE379K/EE379K-Lab4/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/sean/Documents/EE379K/EE379K-Lab4/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# For each of the train/test splits, create a separate random forest with default sklearn parameters\n",
    "# Hint: Ignoring import statements, each random forest can be created in a single line of code\n",
    "# CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# part1\n",
    "p1_class = RandomForestClassifier(random_state=42).fit(train_data, train_labels)\n",
    "# part2\n",
    "p2_class = RandomForestClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN PREDS part1\t\tpart2\n",
      "precision = 0.9955416116248349\t0.9736606050721683\n",
      "   recall = 0.9945562520620257\t0.9848234905971627\n",
      "f1measure = 0.995048687902294\t0.9792102349612498\n",
      " accuracy = 0.9986066601644141\t0.9913515173225526\n",
      "\n",
      "\n",
      "TEST PREDS  part1\t\tpart2\n",
      "precision = 0.842185417856463\t0.7203955112610845\n",
      "   recall = 0.2432908051033876\t0.7573007754495957\n",
      "f1measure = 0.3775227204847037\t0.7383872913734163\n",
      " accuracy = 0.8033403429310902\t0.8890367261996145\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each of the train/test splits and associated random forest, do the following:\n",
    "\n",
    "# 1) Predict labels on the training data\n",
    "# CODE HERE\n",
    "# part1\n",
    "p1_train_preds = p1_class.predict(train_data)\n",
    "\n",
    "# part2\n",
    "p2_train_preds = p2_class.predict(X_train)\n",
    "\n",
    "# 2) Print metrics on the training data; use sklearn's implementation of precision, recall, f1, and accuracy\n",
    "# CODE HERE\n",
    "from sklearn import metrics\n",
    "# part1\n",
    "p1_train_pre = metrics.precision_score(train_labels, p1_train_preds, pos_label='Fuzzers')\n",
    "p1_train_re = metrics.recall_score(train_labels, p1_train_preds, pos_label='Fuzzers')\n",
    "p1_train_f1 = metrics.f1_score(train_labels, p1_train_preds, pos_label='Fuzzers')\n",
    "p1_train_acc = metrics.accuracy_score(train_labels, p1_train_preds)\n",
    "# part2\n",
    "p2_train_pre = metrics.precision_score(y_train, p2_train_preds, pos_label='Fuzzers')\n",
    "p2_train_re = metrics.recall_score(y_train, p2_train_preds, pos_label='Fuzzers')\n",
    "p2_train_f1 = metrics.f1_score(y_train, p2_train_preds, pos_label='Fuzzers')\n",
    "p2_train_acc = metrics.accuracy_score(y_train, p2_train_preds)\n",
    "\n",
    "print('TRAIN PREDS ' + 'part1'           + '\\t\\t' + 'part2')\n",
    "print('precision = ' + str(p1_train_pre) + '\\t' + str(p2_train_pre))\n",
    "print('   recall = ' + str(p1_train_re)  + '\\t' + str(p2_train_re))\n",
    "print('f1measure = ' + str(p1_train_f1)  + '\\t' + str(p2_train_f1))\n",
    "print(' accuracy = ' + str(p1_train_acc) + '\\t' + str(p2_train_acc))\n",
    "print('\\n')\n",
    "\n",
    "# 3) Predict labels on the test data\n",
    "# CODE HERE\n",
    "# part1\n",
    "p1_test_preds = p1_class.predict(test_data)\n",
    "# part2\n",
    "p2_test_preds = p2_class.predict(X_test)\n",
    "\n",
    "# 4) Print metrics on the test data; again, use precision, recall, f1, and accuracy\n",
    "# CODE HERE\n",
    "# part1\n",
    "p1_test_pre = metrics.precision_score(test_labels, p1_test_preds, pos_label='Fuzzers')\n",
    "p1_test_re = metrics.recall_score(test_labels, p1_test_preds, pos_label='Fuzzers')\n",
    "p1_test_f1 = metrics.f1_score(test_labels, p1_test_preds, pos_label='Fuzzers')\n",
    "p1_test_acc = metrics.accuracy_score(test_labels, p1_test_preds)\n",
    "# part2\n",
    "p2_test_pre = metrics.precision_score(y_test, p2_test_preds, pos_label='Fuzzers')\n",
    "p2_test_re = metrics.recall_score(y_test, p2_test_preds, pos_label='Fuzzers')\n",
    "p2_test_f1 = metrics.f1_score(y_test, p2_test_preds, pos_label='Fuzzers')\n",
    "p2_test_acc = metrics.accuracy_score(y_test, p2_test_preds)\n",
    "\n",
    "print('TEST PREDS  ' + 'part1'          + '\\t\\t' + 'part2')\n",
    "print('precision = ' + str(p1_test_pre) + '\\t' + str(p2_test_pre))\n",
    "print('   recall = ' + str(p1_test_re)  + '\\t' + str(p2_test_re))\n",
    "print('f1measure = ' + str(p1_test_f1)  + '\\t' + str(p2_test_f1))\n",
    "print(' accuracy = ' + str(p1_test_acc) + '\\t' + str(p2_test_acc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1) For results using Part 1 data, what is the precision and recall?\n",
    "\n",
    "The precision and recall of the predicted training labels are about 0.9955 and 0.9946, respectively. On the other hand, the precision and recall scores of the predicted testing labels are about 0.8422 and 0.2433, respectively.\n",
    "\n",
    "2) For results using Part 1 data, describe the difference in the results on the training and test data. What does this signify? \n",
    "\n",
    "The results on the training data were all above 0.99 while the results on the test data were much lower for recall and f1 measure. The decrease of each score in general is likely since the test data has entries that are dissimilar to the entries in the training data. In particular, the low recall score in the test predictions for part 1 is likely due to a much greater number of false negatives than false positives, since the only difference in the formulas for precision and recall is false positives and false negatives, respectively. This means that in the test data, the model trained on the training data had a significant number of predictions of 'Normal' attacks that were actually 'Fuzzers' (false negatives), in comparison to the number of false positives, where 'Fuzzers' was predicted, but the true label was 'Normal.'\n",
    "\n",
    "3) What changes in the results on the test data once you combine the data for Part 2? Does this produce a better classifier? Why or why not?\n",
    "\n",
    "The test predictions have a much more balanced spread of scores since the classifier was exposed to data from the test set while training. Overall, this produces a better classifier, since accuracy (a ratio of the correctly predicted labels to the total number of data points) has gone up, despite precision going down. This simply means that in part 2, the number of false positives out of all the positive label predictions is higher compared to the ratio in part 1. In this case, if the organization is worried about fuzzing attacks, then it might be better to have more false positives and be more wary. Additionally, the recall score went up in part 2, meaning that there are relatively less false negatives in the predictions than in part 1. In other words, there are less occurrences of an attack being labeled as 'Normal' when it actually is 'Fuzzers.' Again, this is a good thing when the goal is to separate fuzzing attacks from normal attacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
