{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "The data is separated into three folders: Attack_Data_Master, Training_Data_Master, and Validation_Data_Master\n",
    "These can be found here:\n",
    "data/exercise3/Training_Data_Master\n",
    "data/exercise3/Validation_Data_Master\n",
    "data/exercise3/Attack_Data_Master\n",
    "\n",
    "All of the data in Training_Data_Master and Validation_Data_Master is normal, \n",
    "and all the data in Attack_Data_Master is malicious\n",
    "\n",
    "For the purpose of this exercise, you will ignore the predefined training/validation splits, and simply use Training_Data_Master\n",
    "and Validation_Data_Master as a single pool of normal data\n",
    "\n",
    "As mentioned, each system call trace is stored as a single file.  Treat each system call trace as a separate datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load all the normal system call traces (i.e., everything in Training_Data_Master and Validation_Data_Master)\n",
    "\n",
    "# CODE HERE\n",
    "normal_data = []\n",
    "paths = [\n",
    "    'data/exercise3/Training_Data_Master',\n",
    "    'data/exercise3/Validation_Data_Master',\n",
    "    'data/exercise3/Attack_Data_Master'\n",
    "]\n",
    "for f_name in os.listdir(paths[0]):\n",
    "    with open(paths[0]+'/'+f_name, 'r') as f:\n",
    "        normal_data.append(f.read())\n",
    "for f_name in os.listdir(paths[1]):\n",
    "    with open(paths[1]+'/'+f_name, 'r') as f:\n",
    "        normal_data.append(f.read())\n",
    "\n",
    "# Load all the malicious system call traces (i.e., everything in Attack_Data_Master)\n",
    "\n",
    "# CODE HERE\n",
    "malicious_data = []\n",
    "for dir_name in os.listdir(paths[2]):\n",
    "    for f_name in os.listdir(paths[2]+'/'+dir_name):\n",
    "        with open(paths[2]+'/'+dir_name+'/'+f_name, 'r') as f:\n",
    "            malicious_data.append(f.read())\n",
    "\n",
    "# Hint: A useful way to load this is as one or two Python lists, where each entry in the list corresponds to the text string\n",
    "#       of system calls ids; feel free to use a single list for all the data, or separate lists for malicious versus normal\n",
    "#       data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Tokenize and create a dataset where each datapoint corresponds to (normalized) counts of \n",
    "system call n-grams. Try various sizes of ngrams.\n",
    "\n",
    "Reminder: A sequence of system call IDs that looks like this:\n",
    "'6 6 63 6 42'\n",
    "\n",
    "contains the following 3-grams:\n",
    "'6 6 63'\n",
    "'6 63 6'\n",
    "'63 6 42'\n",
    "\n",
    "Note: There are a number of ways you could code this up, but if you loaded the data\n",
    "as lists of strings, you could consider using some of the feature extraction methods in \n",
    "sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the classdemo notebook for an example of doing this\n",
    "# CODE HERE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer(analyzer='word', ngram_range=(3,3))\n",
    "raw_cnts = cnt_vec.fit_transform(normal_data + malicious_data)\n",
    "features = cnt_vec.get_feature_names()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "all_data = tf_transformer.fit_transform(raw_cnts)\n",
    "raw_labels = [0]*len(normal_data) + [1]*len(malicious_data)\n",
    "all_labels = np.asarray(raw_labels)\n",
    "indices = np.arange(len(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 50% of the data for the training set and the rest for the test set\n",
    "# CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(all_data, all_labels, indices, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/EE379K/EE379K-Lab4/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Please use Logistic Regression for this exercise\n",
    "# Feel free to experiment with the various hyperparameters available to you in sklearn\n",
    "# CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.7789473684210526\n",
      "   recall = 0.6132596685082873\n",
      "f1measure = 0.686244204018547\n",
      " accuracy = 0.9317876344086021\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the test data and predict labels for each data point in the test data\n",
    "# CODE HERE\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the following metrics: precision, recall, f1-measure, and accuracy\n",
    "# CODE HERE\n",
    "from sklearn import metrics\n",
    "precision = metrics.precision_score(y_test, preds)\n",
    "recall = metrics.recall_score(y_test, preds)\n",
    "f1measure = metrics.f1_score(y_test, preds)\n",
    "accuracy = metrics.accuracy_score(y_test, preds)\n",
    "print('precision = ' + str(precision))\n",
    "print('   recall = ' + str(recall))\n",
    "print('f1measure = ' + str(f1measure))\n",
    "print(' accuracy = ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Varying class priors\n",
    "\n",
    "Create several new test datasets where you have randomly subsampled the number of \n",
    "attack datapoints.\n",
    "\n",
    "In particular, create the following datasets:\n",
    "- 10 datasets where 25% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 50% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 75% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 90% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 95% of the attack datapoints are removed from the original test set\n",
    "\n",
    "Report five sets of precision, recall, f1-measure, and accuracy corresponding to the following:\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 25% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 50% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 75% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 90% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 95% of attack datapoints removed\n",
    "\n",
    "Note: You will use the same model trained in part 1 for all of these datasets.  \n",
    "All you are varying is the class priors during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG over 10 rounds of 25.0% attack datapoints removed\n",
      "\tprecision = 0.7256045419634172\n",
      "\t   recall = 0.6090142018895736\n",
      "\tf1measure = 0.6621297346369859\n",
      "\t accuracy = 0.9411021754034359\n",
      "AVG over 10 rounds of 50.0% attack datapoints removed\n",
      "\tprecision = 0.6336376951212832\n",
      "\t   recall = 0.6070608641593191\n",
      "\tf1measure = 0.6198168120078908\n",
      "\t accuracy = 0.9521150519264789\n",
      "AVG over 10 rounds of 75.0% attack datapoints removed\n",
      "\tprecision = 0.45560403781349523\n",
      "\t   recall = 0.6011991178824132\n",
      "\tf1measure = 0.5180762242513929\n",
      "\t accuracy = 0.9636977955454524\n",
      "AVG over 10 rounds of 90.0% attack datapoints removed\n",
      "\tprecision = 0.2610950707613507\n",
      "\t   recall = 0.5646104344963792\n",
      "\tf1measure = 0.3556190278733884\n",
      "\t accuracy = 0.9697453360664197\n",
      "AVG over 10 rounds of 95.0% attack datapoints removed\n",
      "\tprecision = 0.13779985983831045\n",
      "\t   recall = 0.6178381001523416\n",
      "\tf1measure = 0.22397364178211648\n",
      "\t accuracy = 0.9735443529395351\n"
     ]
    }
   ],
   "source": [
    "# Create subsets of the test set by randomly discarding X% of points with label +1\n",
    "# CODE HERE\n",
    "import random\n",
    "random.seed(42)\n",
    "probs = [0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "ids_all = range(len(y_test))\n",
    "for p in probs:\n",
    "    avg_prec = 0\n",
    "    avg_recall = 0\n",
    "    avg_f1 = 0\n",
    "    avg_acc = 0\n",
    "    for _ in range(10):\n",
    "        ids_drop = []\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == 1 and random.random() < p:\n",
    "                ids_drop.append(i)\n",
    "        ids_keep = [x for x in ids_all if x not in ids_drop]\n",
    "        X_test_new = X_test[ids_keep,:]\n",
    "        y_test_new = y_test[ids_keep]\n",
    "        # predict and metrics\n",
    "        preds_new = model.predict(X_test_new)\n",
    "        avg_prec += metrics.precision_score(y_test_new, preds_new)\n",
    "        avg_recall += metrics.recall_score(y_test_new, preds_new)\n",
    "        avg_f1 += metrics.f1_score(y_test_new, preds_new)\n",
    "        avg_acc += metrics.accuracy_score(y_test_new, preds_new)\n",
    "    avg_prec /= 10\n",
    "    avg_recall /= 10\n",
    "    avg_f1 /= 10\n",
    "    avg_acc /= 10\n",
    "    print('AVG over 10 rounds of {}% attack datapoints removed'.format(p*100))\n",
    "    print('\\tprecision = ' + str(avg_prec))\n",
    "    print('\\t   recall = ' + str(avg_recall))\n",
    "    print('\\tf1measure = ' + str(avg_f1))\n",
    "    print('\\t accuracy = ' + str(avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1) In Part 1, what size of ngrams gives the best performance? What are the tradeoffs as you change the size?\n",
    "\n",
    "Using only size 3 n-grams gives the best performance. Using larger n-grams means more features, but seems to reduce scores for all 4 metrics used. This might be because it overfits on the training data rather than being flexible for differences in the testing data.\n",
    "\n",
    "2) In Part 1, how does performance change if we use simple counts as features (i.e., 1-grams) as opposed to counts of 2-grams? What does this tell you about the role of sequences in prediction for this dataset?\n",
    "\n",
    "Performance is worse when using 1-grams compared to 2-grams, and both are worse compared to using 3-grams. This shows that these certain sequences of syscalls should be noted and are good predictors of attacks.\n",
    "\n",
    "3) How does performance change as a function of class prior in Part 2?\n",
    "\n",
    "Although accuracy increases as more attack datapoints are removed, the other 3 metrics decrease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
